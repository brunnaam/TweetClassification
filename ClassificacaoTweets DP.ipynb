{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação de tweets no contexto político\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "#### Brunna de Sousa Pereira Amorim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#carregando os dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "\n",
    "my_data = genfromtxt(\"bow_tweets_carac_dp.csv\", delimiter=',',skip_header=1)\n",
    "#my_data = pd.read_csv(\"bow_tweets_carac_dp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 500000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12004, 'train sequences')\n",
      "(12004, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "#copiar o arquivo gerado no home para a pasta do home\\.keras\\\\datasets\n",
    "x_train = x_test = my_data[:,1:]\n",
    "y_sentiment = my_data[:,0:1]\n",
    "y_train = y_test = [round(y[0]) for y in y_sentiment]\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (12004L, 400L))\n",
      "('x_test shape:', (12004L, 400L))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12004 samples, validate on 12004 samples\n",
      "Epoch 1/2\n",
      "12004/12004 [==============================] - 214s - loss: 0.4317 - acc: 0.7913 - val_loss: 0.3963 - val_acc: 0.8114\n",
      "Epoch 2/2\n",
      "12004/12004 [==============================] - 206s - loss: 0.3762 - acc: 0.8243 - val_loss: 0.3731 - val_acc: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x33ac25c50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score:', 0.37306447601226994)\n",
      "('Test accuracy:', 0.8372209263578807)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.8372209263578807)\n",
      "('Recall:', 0.8372209263578807)\n",
      "('Precision:', 0.8372209263578807)\n",
      "('F1 score:', 0.8372209263578807)\n",
      "('matrix of confusion:', array([[4792, 1107],\n",
      "       [ 847, 5258]]))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x_test)\n",
    "rounded = [(int)(round(x[0])) for x in predicted]\n",
    "matrix = confusion_matrix(y_test, rounded)\n",
    "print('Accuracy:', accuracy_score(y_test, rounded))\n",
    "print( 'Recall:', recall_score(y_test, rounded, average='micro'))\n",
    "print('Precision:', precision_score(y_test, rounded, average='micro'))\n",
    "print('F1 score:',f1_score(y_test, rounded, average='micro'))\n",
    "print('matrix of confusion:',matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brunna\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#dividir o treino e teste de novo\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_treino_feature, X_teste_feature, y_treino_feature, y_teste_feature = train_test_split(x_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8402 samples, validate on 3602 samples\n",
      "Epoch 1/2\n",
      "8402/8402 [==============================] - 132s - loss: 0.3630 - acc: 0.8260 - val_loss: 0.4039 - val_acc: 0.8046\n",
      "Epoch 2/2\n",
      "8402/8402 [==============================] - 148s - loss: 0.3529 - acc: 0.8341 - val_loss: 0.3909 - val_acc: 0.8057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x478c03d68>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_treino_feature, y_treino_feature,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_teste_feature, y_teste_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score:', 0.39087967107125215)\n",
      "('Test accuracy:', 0.80566352033270916)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_teste_feature, y_teste_feature, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_teste_feature)\n",
    "rounded = [(int)(round(x[0])) for x in predicted]\n",
    "matrix = confusion_matrix(y_teste_feature, rounded)\n",
    "print('Accuracy:', accuracy_score(y_teste_feature, rounded))\n",
    "print( 'Recall:', recall_score(y_teste_feature, rounded, average='micro'))\n",
    "print('Precision:', precision_score(y_teste_feature, rounded, average='micro'))\n",
    "print('F1 score:',f1_score(y_teste_feature, rounded, average='micro'))\n",
    "print('matrix of confusion:',matrix)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
